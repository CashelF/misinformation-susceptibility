{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DplZ7auBAqnvKa_DAvbkVh4FKS_3i-Hq",
      "authorship_tag": "ABX9TyOsRiPDxbsIRkb9w23o6aL6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CashelF/misinformation-susceptibility/blob/main/Community_Alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVvd8_ccK_it",
        "outputId": "a7842363-d0d4-4fbd-e31f-ca14ba37a9f5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas networkx sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import networkx as nx\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load a pre-trained sentence transformer model for embeddings\n",
        "transformer_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "depLbtKFLJhf",
        "outputId": "10bf0d15-af36-4f2f-f616-d0cc78953300",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Function to Filter Out Deleted Users"
      ],
      "metadata": {
        "id": "yJEbTeRvbxue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_deleted(data):\n",
        "    \"\"\"Remove rows where the username is '[deleted]'.\"\"\"\n",
        "    return data[data['author'] != \"[deleted]\"]"
      ],
      "metadata": {
        "id": "InylrMsabxLd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recursive Alignment Function"
      ],
      "metadata": {
        "id": "Fy21IiNFb4Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_recursive_alignment(comment_id, comments_dict, parent_alignment):\n",
        "    \"\"\"Recursively determine the alignment of a comment based on its parent.\"\"\"\n",
        "    # If the comment agrees with the parent, it inherits the parent's alignment\n",
        "    if comments_dict[comment_id]['sentiment'] == parent_alignment:\n",
        "        return parent_alignment  # Same alignment as the parent\n",
        "    else:\n",
        "        return -parent_alignment  # Opposite alignment\n"
      ],
      "metadata": {
        "id": "UWImhyY0b3KA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process One Subreddit at a Time"
      ],
      "metadata": {
        "id": "hH0GKTHlTDAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_subreddit(submission_file, comment_file):\n",
        "    \"\"\"Process submissions and comments for a single subreddit and return edges.\"\"\"\n",
        "    # Load submissions and comments, skipping deleted users\n",
        "    submissions = filter_deleted(pd.read_csv(submission_file, dtype={'author_flair_text': str}, low_memory=False))\n",
        "    comments = filter_deleted(pd.read_csv(comment_file, dtype={'parent_id': str, 'link_id': str}, low_memory=False))\n",
        "\n",
        "    # Merge submissions and comments on link_id (removing 't3_' prefix from link_id)\n",
        "    comments['link_id'] = comments['link_id'].str.split('_').str[-1]\n",
        "    merged_data = pd.merge(comments, submissions, left_on='link_id', right_on='id', suffixes=('_comment', '_submission'))\n",
        "\n",
        "    # Generate embeddings for submissions and comments\n",
        "    merged_data['submission_embedding'] = merged_data['title'].apply(\n",
        "        lambda text: transformer_model.encode(text, convert_to_tensor=True)\n",
        "    )\n",
        "    merged_data['comment_embedding'] = merged_data['body'].apply(\n",
        "        lambda text: transformer_model.encode(text, convert_to_tensor=True)\n",
        "    )\n",
        "\n",
        "    # Calculate cosine similarity to determine sentiment (1 = positive, -1 = negative)\n",
        "    merged_data['similarity'] = merged_data.apply(\n",
        "        lambda row: util.cos_sim(row['comment_embedding'], row['submission_embedding']).item(),\n",
        "        axis=1\n",
        "    )\n",
        "    merged_data['sentiment'] = merged_data['similarity'].apply(lambda x: 1 if x >= 0 else -1)\n",
        "\n",
        "    # Create a dictionary of comments with their parent alignment\n",
        "    comments_dict = merged_data.set_index('id_comment').to_dict('index')\n",
        "\n",
        "    # Recursively calculate alignment for each comment based on its parent\n",
        "    for comment_id in comments_dict:\n",
        "        parent_id = comments_dict[comment_id]['parent_id']\n",
        "        if parent_id.startswith('t1_'):  # Ensure the parent is a comment\n",
        "            parent_id = parent_id.split('_')[-1]\n",
        "            if parent_id in comments_dict:  # Parent exists in the data\n",
        "                parent_alignment = comments_dict[parent_id]['sentiment']\n",
        "                comments_dict[comment_id]['alignment_score'] = calculate_recursive_alignment(\n",
        "                    comment_id, comments_dict, parent_alignment\n",
        "                )\n",
        "            else:\n",
        "                # If parent isn't found, assume sentiment is as-is\n",
        "                comments_dict[comment_id]['alignment_score'] = comments_dict[comment_id]['sentiment']\n",
        "        else:\n",
        "            # If it's a top-level comment, alignment is based on the original post\n",
        "            comments_dict[comment_id]['alignment_score'] = comments_dict[comment_id]['sentiment']\n",
        "\n",
        "    # Convert the dictionary back to a DataFrame for processing\n",
        "    aligned_comments = pd.DataFrame.from_dict(comments_dict, orient='index')\n",
        "\n",
        "    # Group by user and subreddit to compute cumulative alignment scores\n",
        "    user_alignment = aligned_comments.groupby(['author_comment', 'subreddit'])['alignment_score'].sum().reset_index()\n",
        "\n",
        "    # Extract edges between subreddit pairs based on shared users\n",
        "    edges = []\n",
        "    for user, group in user_alignment.groupby('author_comment'):\n",
        "        subreddit_pairs = [(a, b) for idx, a in enumerate(group['subreddit']) for b in group['subreddit'][idx + 1:]]\n",
        "\n",
        "        for sub1, sub2 in subreddit_pairs:\n",
        "            align1 = group[group['subreddit'] == sub1]['alignment_score'].values[0]\n",
        "            align2 = group[group['subreddit'] == sub2]['alignment_score'].values[0]\n",
        "\n",
        "            # Determine edge weight based on alignment match\n",
        "            weight = 1 if align1 == align2 else -1\n",
        "            edges.append((sub1, sub2, weight))\n",
        "\n",
        "    # Free memory\n",
        "    del submissions, comments, merged_data, aligned_comments, user_alignment\n",
        "    gc.collect()\n",
        "\n",
        "    return edges"
      ],
      "metadata": {
        "id": "pt20eiMYLLs5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save and Accumulate Edges Across Subreddits"
      ],
      "metadata": {
        "id": "Zp7TmHlPTAQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# Get the list of submission and comment files\n",
        "submission_files = glob.glob('/content/drive/MyDrive/Reddit Misinformation Susceptibility/Data_Sets/*_submissions.csv')\n",
        "comment_files = glob.glob('/content/drive/MyDrive/Reddit Misinformation Susceptibility/Data_Sets/*_comments.csv')\n",
        "\n",
        "# Process each subreddit and save intermediate edges\n",
        "for submission_file, comment_file in zip(submission_files, comment_files):\n",
        "    edges = process_subreddit(submission_file, comment_file)\n",
        "\n",
        "    # Save edges to a temporary file (one per subreddit)\n",
        "    with open(f'{submission_file}_edges.txt', 'w') as f:\n",
        "        for edge in edges:\n",
        "            f.write(f\"{edge[0]},{edge[1]},{edge[2]}\\n\")"
      ],
      "metadata": {
        "id": "XXgL8aBJS5-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Final Graph from Saved Edges"
      ],
      "metadata": {
        "id": "GvjiwsLaTHnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the final graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Load all saved edges and add them to the graph\n",
        "for edge_file in glob.glob('path_to_data/*_edges.txt'):\n",
        "    with open(edge_file, 'r') as f:\n",
        "        for line in f:\n",
        "            sub1, sub2, weight = line.strip().split(',')\n",
        "            weight = int(weight)\n",
        "\n",
        "            # Add edge to the graph\n",
        "            if G.has_edge(sub1, sub2):\n",
        "                G[sub1][sub2]['weight'] += weight\n",
        "            else:\n",
        "                G.add_edge(sub1, sub2, weight=weight)\n",
        "\n",
        "# Save the final graph\n",
        "nx.write_gexf(G, 'subreddit_alignment_graph.gexf')\n"
      ],
      "metadata": {
        "id": "81SsnjpKOI9Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}